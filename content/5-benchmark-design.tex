\chapter{Benchmark Framework Design}

The benchmarking process was designed to compare the performance of various ORM
and SQL query builder packages. As such, it was important to ensure that the
benchmarking framework was developed in the same environment as the packages
themselves. To achieve this, the framework was implemented in TypeScript, the
same language used by the packages being tested.

The benchmarking framework had to be designed to accommodate errors that could
occur during development and testing of the packages. Additionally, it has to
support testing of multiple database schemas and allow for results to be
exported in a variety of formats for further analysis. The resulting
benchmarking framework provides a robust and comprehensive means of comparing
database access packages.

\section{Test Suite and Schema Separation}

The benchmarking framework was designed to support separation of tests into
multiple test suites, a common practice with JavaScript test frameworks such as
Jest \cite{Jest} or Mocha \cite{Mocha}. Test suite separation allows for
organization of tests by subject and contains specifications about the database
schema and data expected to be executed. Input and output parameters must be
typed to test types support, and the framework should provide sufficient
functionality to avoid the need for casting.

The tests are expected to be run simultaneously with snapshots of the database
schema and should not interfere with data used by another test suite. As a
deliberate choice, this limits the scope of each test's modifications over the
database and data. However, it eliminates the need to reset the database to the
original state after each test suite, reducing the time it takes to run the
benchmark. However, such functionality should still be supported, as the
framework should allow for wider range of tests than is expected for
implementation.

\section{Database management}
As the framework operates strictly over a database instance, the framework must
be able to perform maintenance operations. These include initializing the
database with its schema, seeding the database with test data and later tearing
down the schema and replacing it with a different one, depending on which the
test suite requires.

\section{Test type and Error Handling}

The framework needs to support multiple tests to ensure the validity of any
results it produces. If performance is measured, multiple runs can reduce the
impact of statistical anomalies, which can occur due to the innumerable possible
external events.

Along with performance, the correctness of both query types, resulting runtime
types, and the result value are essential. As types are only visible before
compilation, and with typed test suite definitions TypeScript compiler would not
compile the code. Due to this restriction, even incorrect types from the
packages will need to be cast into their expected value. However, even just the
need for such modification means the package is insufficiently supporting type
definitions.

Resulting runtime types and values are validated using the node module
\texttt{node:assert} \cite{NodeAssert}, which provides assertion functions. It
is provided to function with testing frameworks such as mocha, which do not
offer verification functions. Included are even deep equality checking
functions. The main advantage, however, comes from being included in the Node.js
standard library, meaning that no additional package has to be included.

Returning an incorrect result is one of many ways the benchmark test can be
failed; the package can return an unexpected error, or the test is impossible to
perform. Both are fail-states, which the benchmark suite must account for with
error handling. One choice during the design process was that a single failure
would mark the real test as failed, even though other iterations succeeded. If
the package caused the issue, that means the package is not reliable enough and
such problem needs to be marked. If the failure is caused by external issue, for
example database error, the test run can be repeated after triage.

\section{Multi-Framework support}

The benchmarking bootstrap is designed for sequential testing of multiple
packages. This design, rather than separate execution, allows for comprehensive
comparison under the same conditions, ensuring accurate results. As managing the
dependencies could prove problematic if packages had different dependencies
required, npm workspaces \cite{npmWorkspaces} were selected as a project
structure. That way, top-level dependencies of the framework can be separated
from the individual implementations.

As each framework is initialized through its initialization method, abstraction
over the package itself must be created. The initialization also has to account
for delayed connection pool creation. Some packages only start the connection
once the first request is sent to limit the number of open connections for the
database. The database has a limited amount of connections it can support, which
is one way to optimize its usage. The benchmark database can easily handle the
limited connections between the benchmarking framework and the individual
connections; however, it should still contain a method for closing the
connection and destroying the context so subsequent frameworks can access the
same amount of resources.

\section{Reporters - Output options}

An integral part of the design was the inclusion of reporters. Reporters provide
an interface and implementation of multiple output options, enabling the results
to be saved and shown in various formats. Standard test frameworks utilize
reporters to make code coverage or detailed error stack inspectable. The
reporters can interpret the data in different formats with a benchmarking
framework. The reporter interface has to be designed to be extensible, in order
to allow for the easy addition of other output options or data interpretations
in the future.
